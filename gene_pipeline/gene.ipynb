{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3acd77",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "34638385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d80ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37c143f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\user\\Videos\\GeneExp\\data\\labelled_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb505503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>geo_accession</th>\n",
       "      <th>gse_id</th>\n",
       "      <th>ctrl</th>\n",
       "      <th>pert</th>\n",
       "      <th>channel_count</th>\n",
       "      <th>characteristics_ch1</th>\n",
       "      <th>contact_address</th>\n",
       "      <th>contact_city</th>\n",
       "      <th>contact_country</th>\n",
       "      <th>...</th>\n",
       "      <th>extract_protocol_ch2</th>\n",
       "      <th>label_ch2</th>\n",
       "      <th>label_protocol_ch2</th>\n",
       "      <th>molecule_ch2</th>\n",
       "      <th>organism_ch2</th>\n",
       "      <th>source_name_ch2</th>\n",
       "      <th>taxid_ch2</th>\n",
       "      <th>treatment_protocol_ch2</th>\n",
       "      <th>biomaterial_provider_ch2</th>\n",
       "      <th>growth_protocol_ch2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GSM1617977</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GSM1617983</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: Unsorted</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GSM1617982</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GSM1617975</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44high/CD24low</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>GSM1267968</td>\n",
       "      <td>GSE52505</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>tissue: human nasal polyp</td>\n",
       "      <td>148, Gurodong-ro, Guro-gu</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 geo_accession    gse_id  ctrl  pert  channel_count  \\\n",
       "0           0    GSM1617977  GSE66250     0     1              1   \n",
       "1           1    GSM1617983  GSE66250     0     1              1   \n",
       "2           2    GSM1617982  GSE66250     1     0              1   \n",
       "3           3    GSM1617975  GSE66250     0     1              1   \n",
       "4           0    GSM1267968  GSE52505     0     1              1   \n",
       "\n",
       "              characteristics_ch1            contact_address contact_city  \\\n",
       "0  facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n",
       "1          facs sorting: Unsorted                 Am Hubland    Wuerzburg   \n",
       "2  facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n",
       "3  facs sorting: CD44high/CD24low                 Am Hubland    Wuerzburg   \n",
       "4       tissue: human nasal polyp  148, Gurodong-ro, Guro-gu        Seoul   \n",
       "\n",
       "  contact_country  ... extract_protocol_ch2 label_ch2 label_protocol_ch2  \\\n",
       "0         Germany  ...                  NaN       NaN                NaN   \n",
       "1         Germany  ...                  NaN       NaN                NaN   \n",
       "2         Germany  ...                  NaN       NaN                NaN   \n",
       "3         Germany  ...                  NaN       NaN                NaN   \n",
       "4     South Korea  ...                  NaN       NaN                NaN   \n",
       "\n",
       "  molecule_ch2 organism_ch2 source_name_ch2  taxid_ch2 treatment_protocol_ch2  \\\n",
       "0          NaN          NaN             NaN        NaN                    NaN   \n",
       "1          NaN          NaN             NaN        NaN                    NaN   \n",
       "2          NaN          NaN             NaN        NaN                    NaN   \n",
       "3          NaN          NaN             NaN        NaN                    NaN   \n",
       "4          NaN          NaN             NaN        NaN                    NaN   \n",
       "\n",
       "  biomaterial_provider_ch2 growth_protocol_ch2  \n",
       "0                      NaN                 NaN  \n",
       "1                      NaN                 NaN  \n",
       "2                      NaN                 NaN  \n",
       "3                      NaN                 NaN  \n",
       "4                      NaN                 NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80d322a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (623, 53)\n",
      "The columns in the dataset are: ['Unnamed: 0', 'geo_accession', 'gse_id', 'ctrl', 'pert', 'channel_count', 'characteristics_ch1', 'contact_address', 'contact_city', 'contact_country', 'contact_department', 'contact_email', 'contact_institute', 'contact_name', 'contact_state', 'data_processing', 'data_row_count', 'description', 'extract_protocol_ch1', 'growth_protocol_ch1', 'hyb_protocol', 'label_ch1', 'label_protocol_ch1', 'last_update_date', 'molecule_ch1', 'organism_ch1', 'platform_id', 'scan_protocol', 'source_name_ch1', 'status', 'submission_date', 'supplementary_file', 'taxid_ch1', 'title', 'treatment_protocol_ch1', 'type', 'contact_phone', 'contact_laboratory', 'relation', 'contact_fax', 'biomaterial_provider_ch1', 'contact_web_link', 'characteristics_ch2', 'extract_protocol_ch2', 'label_ch2', 'label_protocol_ch2', 'molecule_ch2', 'organism_ch2', 'source_name_ch2', 'taxid_ch2', 'treatment_protocol_ch2', 'biomaterial_provider_ch2', 'growth_protocol_ch2']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 623 entries, 0 to 622\n",
      "Data columns (total 53 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Unnamed: 0                623 non-null    int64  \n",
      " 1   geo_accession             623 non-null    object \n",
      " 2   gse_id                    623 non-null    object \n",
      " 3   ctrl                      623 non-null    int64  \n",
      " 4   pert                      623 non-null    int64  \n",
      " 5   channel_count             623 non-null    int64  \n",
      " 6   characteristics_ch1       623 non-null    object \n",
      " 7   contact_address           623 non-null    object \n",
      " 8   contact_city              623 non-null    object \n",
      " 9   contact_country           623 non-null    object \n",
      " 10  contact_department        503 non-null    object \n",
      " 11  contact_email             394 non-null    object \n",
      " 12  contact_institute         623 non-null    object \n",
      " 13  contact_name              623 non-null    object \n",
      " 14  contact_state             449 non-null    object \n",
      " 15  data_processing           623 non-null    object \n",
      " 16  data_row_count            623 non-null    int64  \n",
      " 17  description               476 non-null    object \n",
      " 18  extract_protocol_ch1      623 non-null    object \n",
      " 19  growth_protocol_ch1       282 non-null    object \n",
      " 20  hyb_protocol              513 non-null    object \n",
      " 21  label_ch1                 513 non-null    object \n",
      " 22  label_protocol_ch1        513 non-null    object \n",
      " 23  last_update_date          623 non-null    object \n",
      " 24  molecule_ch1              623 non-null    object \n",
      " 25  organism_ch1              623 non-null    object \n",
      " 26  platform_id               623 non-null    object \n",
      " 27  scan_protocol             513 non-null    object \n",
      " 28  source_name_ch1           623 non-null    object \n",
      " 29  status                    623 non-null    object \n",
      " 30  submission_date           623 non-null    object \n",
      " 31  supplementary_file        513 non-null    object \n",
      " 32  taxid_ch1                 623 non-null    int64  \n",
      " 33  title                     623 non-null    object \n",
      " 34  treatment_protocol_ch1    298 non-null    object \n",
      " 35  type                      623 non-null    object \n",
      " 36  contact_phone             160 non-null    object \n",
      " 37  contact_laboratory        344 non-null    object \n",
      " 38  relation                  137 non-null    object \n",
      " 39  contact_fax               15 non-null     object \n",
      " 40  biomaterial_provider_ch1  14 non-null     object \n",
      " 41  contact_web_link          9 non-null      object \n",
      " 42  characteristics_ch2       26 non-null     object \n",
      " 43  extract_protocol_ch2      26 non-null     object \n",
      " 44  label_ch2                 26 non-null     object \n",
      " 45  label_protocol_ch2        26 non-null     object \n",
      " 46  molecule_ch2              26 non-null     object \n",
      " 47  organism_ch2              26 non-null     object \n",
      " 48  source_name_ch2           26 non-null     object \n",
      " 49  taxid_ch2                 26 non-null     float64\n",
      " 50  treatment_protocol_ch2    15 non-null     object \n",
      " 51  biomaterial_provider_ch2  4 non-null      object \n",
      " 52  growth_protocol_ch2       9 non-null      object \n",
      "dtypes: float64(1), int64(6), object(46)\n",
      "memory usage: 258.1+ KB\n",
      "The information about the dataset is: None\n",
      "The statistical summary of the dataset is:        Unnamed: 0        ctrl        pert  channel_count  data_row_count  \\\n",
      "count  623.000000  623.000000  623.000000     623.000000      623.000000   \n",
      "mean    18.216693    0.487961    0.512039       1.041734    36633.046549   \n",
      "std     35.473865    0.500257    0.500257       0.200140    21410.142994   \n",
      "min      0.000000    0.000000    0.000000       1.000000        0.000000   \n",
      "25%      1.000000    0.000000    0.000000       1.000000    28231.000000   \n",
      "50%      3.000000    0.000000    1.000000       1.000000    42405.000000   \n",
      "75%      5.000000    1.000000    1.000000       1.000000    54675.000000   \n",
      "max    143.000000    1.000000    1.000000       2.000000   165703.000000   \n",
      "\n",
      "          taxid_ch1     taxid_ch2  \n",
      "count    623.000000     26.000000  \n",
      "mean    9794.518459   9836.076923  \n",
      "std      238.766745    259.863645  \n",
      "min     9544.000000   9544.000000  \n",
      "25%     9606.000000   9606.000000  \n",
      "50%     9606.000000   9848.000000  \n",
      "75%    10090.000000  10090.000000  \n",
      "max    10116.000000  10090.000000  \n",
      "Missing values in each column:\n",
      " Unnamed: 0                    0\n",
      "geo_accession                 0\n",
      "gse_id                        0\n",
      "ctrl                          0\n",
      "pert                          0\n",
      "channel_count                 0\n",
      "characteristics_ch1           0\n",
      "contact_address               0\n",
      "contact_city                  0\n",
      "contact_country               0\n",
      "contact_department          120\n",
      "contact_email               229\n",
      "contact_institute             0\n",
      "contact_name                  0\n",
      "contact_state               174\n",
      "data_processing               0\n",
      "data_row_count                0\n",
      "description                 147\n",
      "extract_protocol_ch1          0\n",
      "growth_protocol_ch1         341\n",
      "hyb_protocol                110\n",
      "label_ch1                   110\n",
      "label_protocol_ch1          110\n",
      "last_update_date              0\n",
      "molecule_ch1                  0\n",
      "organism_ch1                  0\n",
      "platform_id                   0\n",
      "scan_protocol               110\n",
      "source_name_ch1               0\n",
      "status                        0\n",
      "submission_date               0\n",
      "supplementary_file          110\n",
      "taxid_ch1                     0\n",
      "title                         0\n",
      "treatment_protocol_ch1      325\n",
      "type                          0\n",
      "contact_phone               463\n",
      "contact_laboratory          279\n",
      "relation                    486\n",
      "contact_fax                 608\n",
      "biomaterial_provider_ch1    609\n",
      "contact_web_link            614\n",
      "characteristics_ch2         597\n",
      "extract_protocol_ch2        597\n",
      "label_ch2                   597\n",
      "label_protocol_ch2          597\n",
      "molecule_ch2                597\n",
      "organism_ch2                597\n",
      "source_name_ch2             597\n",
      "taxid_ch2                   597\n",
      "treatment_protocol_ch2      608\n",
      "biomaterial_provider_ch2    619\n",
      "growth_protocol_ch2         614\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n",
      "Distribution of the target variable:\n",
      " 0    319\n",
      "1    304\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Describing the dataset\n",
    "print(\"The shape of the dataset is:\", data.shape)\n",
    "print(\"The columns in the dataset are:\", data.columns.tolist())\n",
    "print(\"The information about the dataset is:\", data.info())\n",
    "print(\"The statistical summary of the dataset is:\", data.describe())\n",
    "# To check for missing values\n",
    "print(\"Missing values in each column:\\n\", data.isnull().sum())\n",
    "# To check for duplicate rows\n",
    "print(\"Number of duplicate rows:\", data.duplicated().sum())\n",
    "# To check the distribution of the target variable\n",
    "target = ['ctrl', 'pert']\n",
    "print(\"Distribution of the target variable:\\n\", data[target].value_counts().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "23a8b78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>geo_accession</th>\n",
       "      <th>gse_id</th>\n",
       "      <th>ctrl</th>\n",
       "      <th>pert</th>\n",
       "      <th>channel_count</th>\n",
       "      <th>characteristics_ch1</th>\n",
       "      <th>contact_address</th>\n",
       "      <th>contact_city</th>\n",
       "      <th>contact_country</th>\n",
       "      <th>...</th>\n",
       "      <th>extract_protocol_ch2</th>\n",
       "      <th>label_ch2</th>\n",
       "      <th>label_protocol_ch2</th>\n",
       "      <th>molecule_ch2</th>\n",
       "      <th>organism_ch2</th>\n",
       "      <th>source_name_ch2</th>\n",
       "      <th>taxid_ch2</th>\n",
       "      <th>treatment_protocol_ch2</th>\n",
       "      <th>biomaterial_provider_ch2</th>\n",
       "      <th>growth_protocol_ch2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GSM1617977</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GSM1617983</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: Unsorted</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GSM1617982</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GSM1617975</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facs sorting: CD44high/CD24low</td>\n",
       "      <td>Am Hubland</td>\n",
       "      <td>Wuerzburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>GSM1267968</td>\n",
       "      <td>GSE52505</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>tissue: human nasal polyp</td>\n",
       "      <td>148, Gurodong-ro, Guro-gu</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 geo_accession    gse_id  ctrl  pert  channel_count  \\\n",
       "0           0    GSM1617977  GSE66250     0     1              1   \n",
       "1           1    GSM1617983  GSE66250     0     1              1   \n",
       "2           2    GSM1617982  GSE66250     1     0              1   \n",
       "3           3    GSM1617975  GSE66250     0     1              1   \n",
       "4           0    GSM1267968  GSE52505     0     1              1   \n",
       "\n",
       "              characteristics_ch1            contact_address contact_city  \\\n",
       "0  facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n",
       "1          facs sorting: Unsorted                 Am Hubland    Wuerzburg   \n",
       "2  facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n",
       "3  facs sorting: CD44high/CD24low                 Am Hubland    Wuerzburg   \n",
       "4       tissue: human nasal polyp  148, Gurodong-ro, Guro-gu        Seoul   \n",
       "\n",
       "  contact_country  ... extract_protocol_ch2 label_ch2 label_protocol_ch2  \\\n",
       "0         Germany  ...                  NaN       NaN                NaN   \n",
       "1         Germany  ...                  NaN       NaN                NaN   \n",
       "2         Germany  ...                  NaN       NaN                NaN   \n",
       "3         Germany  ...                  NaN       NaN                NaN   \n",
       "4     South Korea  ...                  NaN       NaN                NaN   \n",
       "\n",
       "  molecule_ch2 organism_ch2 source_name_ch2  taxid_ch2 treatment_protocol_ch2  \\\n",
       "0          NaN          NaN             NaN        NaN                    NaN   \n",
       "1          NaN          NaN             NaN        NaN                    NaN   \n",
       "2          NaN          NaN             NaN        NaN                    NaN   \n",
       "3          NaN          NaN             NaN        NaN                    NaN   \n",
       "4          NaN          NaN             NaN        NaN                    NaN   \n",
       "\n",
       "  biomaterial_provider_ch2 growth_protocol_ch2  \n",
       "0                      NaN                 NaN  \n",
       "1                      NaN                 NaN  \n",
       "2                      NaN                 NaN  \n",
       "3                      NaN                 NaN  \n",
       "4                      NaN                 NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy of the original data\n",
    "data_copy = data.copy()\n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f6655b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet of the cleaned data:\n",
      "   geo_accession    gse_id  ctrl  pert  channel_count  \\\n",
      "0    GSM1617977  GSE66250     0     1              1   \n",
      "1    GSM1617983  GSE66250     0     1              1   \n",
      "2    GSM1617982  GSE66250     1     0              1   \n",
      "3    GSM1617975  GSE66250     0     1              1   \n",
      "4    GSM1267968  GSE52505     0     1              1   \n",
      "\n",
      "              characteristics_ch1 contact_country  \\\n",
      "0  facs sorting: CD44low/CD24high         Germany   \n",
      "1          facs sorting: Unsorted         Germany   \n",
      "2  facs sorting: CD44low/CD24high         Germany   \n",
      "3  facs sorting: CD44high/CD24low         Germany   \n",
      "4       tissue: human nasal polyp     South Korea   \n",
      "\n",
      "                             contact_department    contact_name  \\\n",
      "0  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
      "1  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
      "2  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
      "3  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
      "4             Department of otorhinolaryngology  Heung-Man,,Lee   \n",
      "\n",
      "                                     data_processing  ...  molecule_ch1  \\\n",
      "0  Basecalling was performed with the real time a...  ...     polyA RNA   \n",
      "1  Basecalling was performed with the real time a...  ...     polyA RNA   \n",
      "2  Basecalling was performed with the real time a...  ...     polyA RNA   \n",
      "3  Basecalling was performed with the real time a...  ...     polyA RNA   \n",
      "4  All data normalization and selection of fold-c...  ...     total RNA   \n",
      "\n",
      "   organism_ch1 platform_id  \\\n",
      "0  Homo sapiens    GPL10999   \n",
      "1  Homo sapiens    GPL10999   \n",
      "2  Homo sapiens    GPL10999   \n",
      "3  Homo sapiens    GPL10999   \n",
      "4  Homo sapiens    GPL13497   \n",
      "\n",
      "                                       scan_protocol        source_name_ch1  \\\n",
      "0                                                NaN  Sorted HMLE cell line   \n",
      "1                                                NaN         IMEC cell line   \n",
      "2                                                NaN  Sorted HMLE cell line   \n",
      "3                                                NaN  Sorted HMLE cell line   \n",
      "4  The hybridized images were scanned using Agile...            nasal polyp   \n",
      "\n",
      "  submission_date taxid_ch1                                            title  \\\n",
      "0     Feb 24 2015      9606                                 rep1_cd44low_dox   \n",
      "1     Feb 24 2015      9606                               rep1_imecs_myc_dox   \n",
      "2     Feb 24 2015      9606                                rep2_cd44low_etoh   \n",
      "3     Feb 24 2015      9606                                rep1_cd44high_dox   \n",
      "4     Nov 19 2013      9606  lipopolysaccharide treated sample replication 2   \n",
      "\n",
      "                              treatment_protocol_ch1 type  \n",
      "0  Cell were infected with the indicated doxycycl...  SRA  \n",
      "1  Cell were infected with the indicated doxycycl...  SRA  \n",
      "2  Cell were infected with the indicated doxycycl...  SRA  \n",
      "3  Cell were infected with the indicated doxycycl...  SRA  \n",
      "4  Freshly drawn nasal fibroblasts were treated w...  RNA  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns without data entries\n",
    "cols = ['contact_phone', 'contact_laboratory', 'relation', 'contact_fax', 'biomaterial_provider_ch1', 'contact_web_link', 'characteristics_ch2', 'extract_protocol_ch2', 'label_ch2', 'label_protocol_ch2', 'molecule_ch2', 'organism_ch2', 'source_name_ch2', 'taxid_ch2', 'treatment_protocol_ch2', 'biomaterial_provider_ch2', 'growth_protocol_ch2', 'Unnamed: 0', 'contact_address', 'contact_city', 'contact_email', 'contact_institute', 'contact_state', 'supplementary_file', 'status']\n",
    "data_df1 = data_copy.drop(columns=cols, axis=1)\n",
    "print(\"Snippet of the cleaned data:\\n\", data_df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5717618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(623, 28)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df1.columns\n",
    "data_df1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4303a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['growth_protocol_ch1']\n",
      "['growth_protocol_ch1', 'treatment_protocol_ch1']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 623 entries, 0 to 622\n",
      "Data columns (total 26 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   geo_accession         623 non-null    object\n",
      " 1   gse_id                623 non-null    object\n",
      " 2   ctrl                  623 non-null    int64 \n",
      " 3   pert                  623 non-null    int64 \n",
      " 4   channel_count         623 non-null    int64 \n",
      " 5   characteristics_ch1   623 non-null    object\n",
      " 6   contact_country       623 non-null    object\n",
      " 7   contact_department    503 non-null    object\n",
      " 8   contact_name          623 non-null    object\n",
      " 9   data_processing       623 non-null    object\n",
      " 10  data_row_count        623 non-null    int64 \n",
      " 11  description           476 non-null    object\n",
      " 12  extract_protocol_ch1  623 non-null    object\n",
      " 13  hyb_protocol          513 non-null    object\n",
      " 14  label_ch1             513 non-null    object\n",
      " 15  label_protocol_ch1    513 non-null    object\n",
      " 16  last_update_date      623 non-null    object\n",
      " 17  molecule_ch1          623 non-null    object\n",
      " 18  organism_ch1          623 non-null    object\n",
      " 19  platform_id           623 non-null    object\n",
      " 20  scan_protocol         513 non-null    object\n",
      " 21  source_name_ch1       623 non-null    object\n",
      " 22  submission_date       623 non-null    object\n",
      " 23  taxid_ch1             623 non-null    int64 \n",
      " 24  title                 623 non-null    object\n",
      " 25  type                  623 non-null    object\n",
      "dtypes: int64(5), object(21)\n",
      "memory usage: 126.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# # Drop columns with more than 50% missing values\n",
    "# threshold = len(data) * 0.5\n",
    "# data = data.loc[:, data.isnull().sum() < threshold]\n",
    "\n",
    "threshold_null = data_df1.shape[0] // 2\n",
    "\n",
    "cols_drop = []\n",
    "for col in data_df1.columns:\n",
    "    if data_df1.loc[ : , col].isnull().sum() > threshold_null:\n",
    "        cols_drop.append(col)\n",
    "        print(cols_drop)\n",
    "data_df1.drop(columns=cols_drop, axis=1, inplace=True)\n",
    "data_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "942448a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns in the dataset: ['geo_accession', 'gse_id', 'characteristics_ch1', 'contact_country', 'contact_department', 'contact_name', 'data_processing', 'description', 'extract_protocol_ch1', 'hyb_protocol', 'label_ch1', 'label_protocol_ch1', 'last_update_date', 'molecule_ch1', 'organism_ch1', 'platform_id', 'scan_protocol', 'source_name_ch1', 'submission_date', 'title', 'type']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_accession</th>\n",
       "      <th>gse_id</th>\n",
       "      <th>characteristics_ch1</th>\n",
       "      <th>contact_country</th>\n",
       "      <th>contact_department</th>\n",
       "      <th>contact_name</th>\n",
       "      <th>data_processing</th>\n",
       "      <th>description</th>\n",
       "      <th>extract_protocol_ch1</th>\n",
       "      <th>hyb_protocol</th>\n",
       "      <th>...</th>\n",
       "      <th>label_protocol_ch1</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>molecule_ch1</th>\n",
       "      <th>organism_ch1</th>\n",
       "      <th>platform_id</th>\n",
       "      <th>scan_protocol</th>\n",
       "      <th>source_name_ch1</th>\n",
       "      <th>submission_date</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM1617977</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Chair for Biochemistry and Molecular Biology</td>\n",
       "      <td>Martin,,Eilers</td>\n",
       "      <td>Basecalling was performed with the real time a...</td>\n",
       "      <td>cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...</td>\n",
       "      <td>Total RNA was extracted using the RNeasy Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May 15 2019</td>\n",
       "      <td>polyA RNA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GPL10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sorted HMLE cell line</td>\n",
       "      <td>Feb 24 2015</td>\n",
       "      <td>rep1_cd44low_dox</td>\n",
       "      <td>SRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM1617983</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>facs sorting: Unsorted</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Chair for Biochemistry and Molecular Biology</td>\n",
       "      <td>Martin,,Eilers</td>\n",
       "      <td>Basecalling was performed with the real time a...</td>\n",
       "      <td>imecs_myc_dox_vs_vector_dox.txt</td>\n",
       "      <td>Total RNA was extracted using the RNeasy Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May 15 2019</td>\n",
       "      <td>polyA RNA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GPL10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMEC cell line</td>\n",
       "      <td>Feb 24 2015</td>\n",
       "      <td>rep1_imecs_myc_dox</td>\n",
       "      <td>SRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM1617982</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>facs sorting: CD44low/CD24high</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Chair for Biochemistry and Molecular Biology</td>\n",
       "      <td>Martin,,Eilers</td>\n",
       "      <td>Basecalling was performed with the real time a...</td>\n",
       "      <td>cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...</td>\n",
       "      <td>Total RNA was extracted using the RNeasy Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May 15 2019</td>\n",
       "      <td>polyA RNA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GPL10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sorted HMLE cell line</td>\n",
       "      <td>Feb 24 2015</td>\n",
       "      <td>rep2_cd44low_etoh</td>\n",
       "      <td>SRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM1617975</td>\n",
       "      <td>GSE66250</td>\n",
       "      <td>facs sorting: CD44high/CD24low</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Chair for Biochemistry and Molecular Biology</td>\n",
       "      <td>Martin,,Eilers</td>\n",
       "      <td>Basecalling was performed with the real time a...</td>\n",
       "      <td>cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...</td>\n",
       "      <td>Total RNA was extracted using the RNeasy Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May 15 2019</td>\n",
       "      <td>polyA RNA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GPL10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sorted HMLE cell line</td>\n",
       "      <td>Feb 24 2015</td>\n",
       "      <td>rep1_cd44high_dox</td>\n",
       "      <td>SRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM1267968</td>\n",
       "      <td>GSE52505</td>\n",
       "      <td>tissue: human nasal polyp</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Department of otorhinolaryngology</td>\n",
       "      <td>Heung-Man,,Lee</td>\n",
       "      <td>All data normalization and selection of fold-c...</td>\n",
       "      <td>Gene expression of LPS-stimulated nasal fibrob...</td>\n",
       "      <td>Nasal fibroblasts were exposed to LPS (10 μg/m...</td>\n",
       "      <td>After checking labeling efficiency, fragmentat...</td>\n",
       "      <td>...</td>\n",
       "      <td>Amplified and labeled cRNA was purified on cRN...</td>\n",
       "      <td>Dec 31 2015</td>\n",
       "      <td>total RNA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GPL13497</td>\n",
       "      <td>The hybridized images were scanned using Agile...</td>\n",
       "      <td>nasal polyp</td>\n",
       "      <td>Nov 19 2013</td>\n",
       "      <td>lipopolysaccharide treated sample replication 2</td>\n",
       "      <td>RNA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  geo_accession    gse_id             characteristics_ch1 contact_country  \\\n",
       "0    GSM1617977  GSE66250  facs sorting: CD44low/CD24high         Germany   \n",
       "1    GSM1617983  GSE66250          facs sorting: Unsorted         Germany   \n",
       "2    GSM1617982  GSE66250  facs sorting: CD44low/CD24high         Germany   \n",
       "3    GSM1617975  GSE66250  facs sorting: CD44high/CD24low         Germany   \n",
       "4    GSM1267968  GSE52505       tissue: human nasal polyp     South Korea   \n",
       "\n",
       "                             contact_department    contact_name  \\\n",
       "0  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
       "1  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
       "2  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
       "3  Chair for Biochemistry and Molecular Biology  Martin,,Eilers   \n",
       "4             Department of otorhinolaryngology  Heung-Man,,Lee   \n",
       "\n",
       "                                     data_processing  \\\n",
       "0  Basecalling was performed with the real time a...   \n",
       "1  Basecalling was performed with the real time a...   \n",
       "2  Basecalling was performed with the real time a...   \n",
       "3  Basecalling was performed with the real time a...   \n",
       "4  All data normalization and selection of fold-c...   \n",
       "\n",
       "                                         description  \\\n",
       "0  cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...   \n",
       "1                    imecs_myc_dox_vs_vector_dox.txt   \n",
       "2  cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...   \n",
       "3  cd44high_dox_vs_etoh.txt and cd44high_etoh_vs_...   \n",
       "4  Gene expression of LPS-stimulated nasal fibrob...   \n",
       "\n",
       "                                extract_protocol_ch1  \\\n",
       "0  Total RNA was extracted using the RNeasy Mini ...   \n",
       "1  Total RNA was extracted using the RNeasy Mini ...   \n",
       "2  Total RNA was extracted using the RNeasy Mini ...   \n",
       "3  Total RNA was extracted using the RNeasy Mini ...   \n",
       "4  Nasal fibroblasts were exposed to LPS (10 μg/m...   \n",
       "\n",
       "                                        hyb_protocol  ...  \\\n",
       "0                                                NaN  ...   \n",
       "1                                                NaN  ...   \n",
       "2                                                NaN  ...   \n",
       "3                                                NaN  ...   \n",
       "4  After checking labeling efficiency, fragmentat...  ...   \n",
       "\n",
       "                                  label_protocol_ch1 last_update_date  \\\n",
       "0                                                NaN      May 15 2019   \n",
       "1                                                NaN      May 15 2019   \n",
       "2                                                NaN      May 15 2019   \n",
       "3                                                NaN      May 15 2019   \n",
       "4  Amplified and labeled cRNA was purified on cRN...      Dec 31 2015   \n",
       "\n",
       "  molecule_ch1  organism_ch1 platform_id  \\\n",
       "0    polyA RNA  Homo sapiens    GPL10999   \n",
       "1    polyA RNA  Homo sapiens    GPL10999   \n",
       "2    polyA RNA  Homo sapiens    GPL10999   \n",
       "3    polyA RNA  Homo sapiens    GPL10999   \n",
       "4    total RNA  Homo sapiens    GPL13497   \n",
       "\n",
       "                                       scan_protocol        source_name_ch1  \\\n",
       "0                                                NaN  Sorted HMLE cell line   \n",
       "1                                                NaN         IMEC cell line   \n",
       "2                                                NaN  Sorted HMLE cell line   \n",
       "3                                                NaN  Sorted HMLE cell line   \n",
       "4  The hybridized images were scanned using Agile...            nasal polyp   \n",
       "\n",
       "  submission_date                                            title type  \n",
       "0     Feb 24 2015                                 rep1_cd44low_dox  SRA  \n",
       "1     Feb 24 2015                               rep1_imecs_myc_dox  SRA  \n",
       "2     Feb 24 2015                                rep2_cd44low_etoh  SRA  \n",
       "3     Feb 24 2015                                rep1_cd44high_dox  SRA  \n",
       "4     Nov 19 2013  lipopolysaccharide treated sample replication 2  RNA  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Detecting more object columns\n",
    "obj_cols = [ col for col in data_df1.columns if data_df1[col].dtype == 'object']\n",
    "print(\"Object columns in the dataset:\", obj_cols)\n",
    "data_df1[obj_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39434166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 623 entries, 0 to 622\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   geo_accession         623 non-null    object\n",
      " 1   gse_id                623 non-null    object\n",
      " 2   ctrl                  623 non-null    int64 \n",
      " 3   pert                  623 non-null    int64 \n",
      " 4   channel_count         623 non-null    int64 \n",
      " 5   characteristics_ch1   623 non-null    object\n",
      " 6   contact_name          623 non-null    object\n",
      " 7   data_row_count        623 non-null    int64 \n",
      " 8   extract_protocol_ch1  623 non-null    object\n",
      " 9   label_ch1             513 non-null    object\n",
      " 10  last_update_date      623 non-null    object\n",
      " 11  molecule_ch1          623 non-null    object\n",
      " 12  organism_ch1          623 non-null    object\n",
      " 13  source_name_ch1       623 non-null    object\n",
      " 14  taxid_ch1             623 non-null    int64 \n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 73.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dropping some object columns\n",
    "obj_cols_drop = ['data_processing', 'description', 'platform_id', 'title', 'submission_date', 'hyb_protocol', 'label_protocol_ch1', 'scan_protocol', 'contact_department', 'contact_country','type']\n",
    "data_df1.drop(columns=obj_cols_drop, axis=1, inplace=True)\n",
    "data_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f1cf4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for modeling\n",
    "y = data_df1['ctrl']\n",
    "X = data_df1.drop(columns=['ctrl', 'pert', 'gse_id', 'geo_accession'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "623e76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['characteristics_ch1', 'extract_protocol_ch1', 'label_ch1', 'molecule_ch1', 'organism_ch1', 'source_name_ch1']\n"
     ]
    }
   ],
   "source": [
    "obj = [cols for cols in data_df1.columns if data_df1[cols].dtype == 'object']\n",
    "obj.remove('geo_accession')\n",
    "obj.remove('gse_id')\n",
    "obj.remove('contact_name')\n",
    "obj.remove('last_update_date')\n",
    "# obj.remove('label_protocol_ch1')\n",
    "# obj.remove('hyb_protocol')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "28da323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_df):\n",
    "    data_df1['cleaned'] = ''\n",
    "    \n",
    "    # Initializing Stopwords and Lemmatization objects\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Pattern to detect characters which are not alphabets or numbers so they can removed\n",
    "    alpha_or_numeric = \"[^a-zA-Z0-9- ]\"\n",
    "    \n",
    "    for index, row in tqdm(data_df.iterrows(), total = data_df.shape[0]):\n",
    "        sample = row['feature']\n",
    "        \n",
    "        # Replacing characters which are not alphabets or numbers with blank space and ...\n",
    "        # ... changing text to lowercase. These two steps are for cleaning text data, ...\n",
    "        # ... you can add more on top of this to make your data cleaner.\n",
    "        pre_txt = re.sub(alpha_or_numeric, \" \", sample)\n",
    "        pre_txt = sample.lower()\n",
    "        \n",
    "        # Removing stop words and lemmatizing different words in preprocessed text ...\n",
    "        # ... and making the final processed text\n",
    "        sample_words = [wordnet_lemm.lemmatize(w) for w in pre_txt.split() \\\n",
    "            if w not in stop_words and len(w)>1]\n",
    "        pre_proc_ver = ' '.join(sample_words)\n",
    "        \n",
    "        data_df.loc[index, 'cleaned'] = pre_proc_ver\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ca7cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 623/623 [00:00<00:00, 834.53it/s] \n",
      "100%|██████████| 623/623 [00:00<00:00, 834.53it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facs sorting: cd44low/cd24high total rna extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facs sorting: unsorted total rna extracted usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facs sorting: cd44low/cd24high total rna extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facs sorting: cd44high/cd24low total rna extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tissue: human nasal polyp nasal fibroblast exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned\n",
       "0  facs sorting: cd44low/cd24high total rna extra...\n",
       "1  facs sorting: unsorted total rna extracted usi...\n",
       "2  facs sorting: cd44low/cd24high total rna extra...\n",
       "3  facs sorting: cd44high/cd24low total rna extra...\n",
       "4  tissue: human nasal polyp nasal fibroblast exp..."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = data_df1.loc[ : , obj]\n",
    "df_text['feature'] = data_df1.apply(lambda row: ' '.join([str(row[i]) for i in obj]), axis=1)\n",
    "df_text = preprocess(pd.DataFrame(df_text))\n",
    "\n",
    "# Removing the Unnecessary Features\n",
    "df_text = pd.DataFrame(df_text['cleaned'])\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7bae5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 1093)\n"
     ]
    }
   ],
   "source": [
    "# ... downloaded when importing libraries.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vect = TfidfVectorizer(analyzer=\"word\", preprocessor=None, stop_words=list(stop_words), max_features=4000)\n",
    "df_text = vect.fit_transform(df_text['cleaned'])\n",
    "df = pd.DataFrame(df_text.toarray())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8771a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 161)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=.99, svd_solver='full')\n",
    "df = pca.fit_transform(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7629111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 161) (125, 161) (498,) (125,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f9ad6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "        'logistic_regression': LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'random_forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'gradient_boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'xgboost': xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        'lightgbm': lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5aea1d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training logistic_regression...\n",
      "============================================================\n",
      "\n",
      "logistic_regression:\n",
      "  Training F1-Score:   0.9000\n",
      "  Validation F1-Score: 0.6984\n",
      "  Training Accuracy:   0.8996\n",
      "  Validation Accuracy: 0.6960\n",
      "  Confusion Matrix:\n",
      " [[43 22]\n",
      " [16 44]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69        65\n",
      "           1       0.67      0.73      0.70        60\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.70      0.70      0.70       125\n",
      "weighted avg       0.70      0.70      0.70       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training random_forest...\n",
      "============================================================\n",
      "\n",
      "random_forest:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7967\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.8000\n",
      "  Confusion Matrix:\n",
      " [[51 14]\n",
      " [11 49]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80        65\n",
      "           1       0.78      0.82      0.80        60\n",
      "\n",
      "    accuracy                           0.80       125\n",
      "   macro avg       0.80      0.80      0.80       125\n",
      "weighted avg       0.80      0.80      0.80       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training gradient_boosting...\n",
      "============================================================\n",
      "\n",
      "random_forest:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7967\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.8000\n",
      "  Confusion Matrix:\n",
      " [[51 14]\n",
      " [11 49]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80        65\n",
      "           1       0.78      0.82      0.80        60\n",
      "\n",
      "    accuracy                           0.80       125\n",
      "   macro avg       0.80      0.80      0.80       125\n",
      "weighted avg       0.80      0.80      0.80       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training gradient_boosting...\n",
      "============================================================\n",
      "\n",
      "gradient_boosting:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7797\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.7920\n",
      "  Confusion Matrix:\n",
      " [[53 12]\n",
      " [14 46]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80        65\n",
      "           1       0.79      0.77      0.78        60\n",
      "\n",
      "    accuracy                           0.79       125\n",
      "   macro avg       0.79      0.79      0.79       125\n",
      "weighted avg       0.79      0.79      0.79       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training xgboost...\n",
      "============================================================\n",
      "\n",
      "gradient_boosting:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7797\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.7920\n",
      "  Confusion Matrix:\n",
      " [[53 12]\n",
      " [14 46]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80        65\n",
      "           1       0.79      0.77      0.78        60\n",
      "\n",
      "    accuracy                           0.79       125\n",
      "   macro avg       0.79      0.79      0.79       125\n",
      "weighted avg       0.79      0.79      0.79       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training xgboost...\n",
      "============================================================\n",
      "\n",
      "xgboost:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7642\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.7680\n",
      "  Confusion Matrix:\n",
      " [[49 16]\n",
      " [13 47]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77        65\n",
      "           1       0.75      0.78      0.76        60\n",
      "\n",
      "    accuracy                           0.77       125\n",
      "   macro avg       0.77      0.77      0.77       125\n",
      "weighted avg       0.77      0.77      0.77       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training lightgbm...\n",
      "============================================================\n",
      "\n",
      "xgboost:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7642\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.7680\n",
      "  Confusion Matrix:\n",
      " [[49 16]\n",
      " [13 47]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77        65\n",
      "           1       0.75      0.78      0.76        60\n",
      "\n",
      "    accuracy                           0.77       125\n",
      "   macro avg       0.77      0.77      0.77       125\n",
      "weighted avg       0.77      0.77      0.77       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training lightgbm...\n",
      "============================================================\n",
      "\n",
      "lightgbm:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7934\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.8000\n",
      "  Confusion Matrix:\n",
      " [[52 13]\n",
      " [12 48]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81        65\n",
      "           1       0.79      0.80      0.79        60\n",
      "\n",
      "    accuracy                           0.80       125\n",
      "   macro avg       0.80      0.80      0.80       125\n",
      "weighted avg       0.80      0.80      0.80       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY - ALL MODELS\n",
      "============================================================\n",
      "              model  train_f1   val_f1  train_acc  val_acc\n",
      "logistic_regression   0.90000 0.698413   0.899598    0.696\n",
      "      random_forest   0.99389 0.796748   0.993976    0.800\n",
      "  gradient_boosting   0.99389 0.779661   0.993976    0.792\n",
      "            xgboost   0.99389 0.764228   0.993976    0.768\n",
      "           lightgbm   0.99389 0.793388   0.993976    0.800\n",
      "\n",
      "lightgbm:\n",
      "  Training F1-Score:   0.9939\n",
      "  Validation F1-Score: 0.7934\n",
      "  Training Accuracy:   0.9940\n",
      "  Validation Accuracy: 0.8000\n",
      "  Confusion Matrix:\n",
      " [[52 13]\n",
      " [12 48]]\n",
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81        65\n",
      "           1       0.79      0.80      0.79        60\n",
      "\n",
      "    accuracy                           0.80       125\n",
      "   macro avg       0.80      0.80      0.80       125\n",
      "weighted avg       0.80      0.80      0.80       125\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY - ALL MODELS\n",
      "============================================================\n",
      "              model  train_f1   val_f1  train_acc  val_acc\n",
      "logistic_regression   0.90000 0.698413   0.899598    0.696\n",
      "      random_forest   0.99389 0.796748   0.993976    0.800\n",
      "  gradient_boosting   0.99389 0.779661   0.993976    0.792\n",
      "            xgboost   0.99389 0.764228   0.993976    0.768\n",
      "           lightgbm   0.99389 0.793388   0.993976    0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Metrics for this model\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    conf_m = confusion_matrix(y_val, y_val_pred)\n",
    "    class_report = classification_report(y_val, y_val_pred)\n",
    "    \n",
    "    # Print metrics for this model\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Training F1-Score:   {train_f1:.4f}\")\n",
    "    print(f\"  Validation F1-Score: {val_f1:.4f}\")\n",
    "    print(f\"  Training Accuracy:   {train_acc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(\"  Confusion Matrix:\\n\", conf_m)\n",
    "    print(\"  Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'classification_report': class_report\n",
    "    })\n",
    "\n",
    "# Display summary table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY - ALL MODELS\")\n",
    "print('='*60)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.drop(columns=['classification_report']).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8d480670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unlabeled data: C:\\Users\\user\\Videos\\GeneExp\\data\\unlabelled_train_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19456/19456 [00:12<00:00, 1548.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled features prepared: (19456, 161)\n",
      "Using best model from results: random_forest\n",
      "Total unlabeled samples: 19456 | High-confidence (>= 0.6): 320\n",
      "Saved 320 pseudo-labeled samples to: pseudo_labels.csv\n",
      "Retraining random_forest on augmented data (orig=498, pseudo=320)...\n",
      "Retrained with sample_weight\n",
      "Validation F1 after pseudo-label retrain: 0.7581\n",
      "Validation Acc after pseudo-label retrain: 0.7600\n",
      "Pseudo-labeling iteration complete. Review saved pseudo-labels and validation metrics before repeating.\n"
     ]
    }
   ],
   "source": [
    "# --- PSEUDO-LABELING: One iteration ---\n",
    "# This cell: 1) loads unlabeled data, 2) preprocesses with your existing `vect` and `pca`,\n",
    "# 3) predicts with the best model (selected from `results_df`), 4) keeps high-confidence\n",
    "#    predictions as pseudo-labels, 5) saves them and 6) retrains the best model on the\n",
    "#    augmented training set.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "UNLABELED_FILE = r'C:\\Users\\user\\Videos\\GeneExp\\data\\unlabelled_train_data.csv'  # change if different\n",
    "CONF_THRESH = 0.6\n",
    "MAX_PSEUDO = 5000\n",
    "PSEUDO_WEIGHT = 0.5\n",
    "PSEUDO_OUTPUT = 'pseudo_labels.csv'\n",
    "\n",
    "if not os.path.exists(UNLABELED_FILE):\n",
    "    print(f\"Unlabeled file not found: {UNLABELED_FILE}. Update the path and re-run.\")\n",
    "else:\n",
    "    print(f\"Loading unlabeled data: {UNLABELED_FILE}\")\n",
    "    df_unl = pd.read_csv(UNLABELED_FILE)\n",
    "\n",
    "    # Build the same 'feature' used earlier: concatenate object columns in `obj`\n",
    "    try:\n",
    "        df_unl_features = df_unl[obj].astype(str).agg(' '.join, axis=1)\n",
    "    except Exception as e:\n",
    "        # Fallback: try building from columns present in training\n",
    "        print(\"Warning building text features using `obj` failed:\", e)\n",
    "        common = [c for c in obj if c in df_unl.columns]\n",
    "        if not common:\n",
    "            raise RuntimeError(\"No overlapping feature columns found between labeled and unlabeled data.\")\n",
    "        df_unl_features = df_unl[common].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    df_unl_feat = pd.DataFrame({'feature': df_unl_features})\n",
    "\n",
    "    # Reuse the same text preprocessing function used earlier\n",
    "    df_unl_proc = preprocess(df_unl_feat)\n",
    "    docs = df_unl_proc['cleaned'].astype(str)\n",
    "\n",
    "    # Vectorize and project with existing transformers\n",
    "    X_unl_tfidf = vect.transform(docs)\n",
    "    X_unl_array = X_unl_tfidf.toarray()\n",
    "    X_unl = pca.transform(X_unl_array)\n",
    "\n",
    "    print(f\"Unlabeled features prepared: {X_unl.shape}\")\n",
    "\n",
    "    # Select best model from results_df if available\n",
    "    try:\n",
    "        best_name = results_df.sort_values('val_f1', ascending=False).iloc[0]['model']\n",
    "        print(f\"Using best model from results: {best_name}\")\n",
    "    except Exception:\n",
    "        best_name = list(models.keys())[0]\n",
    "        print(f\"Could not read results_df; defaulting to first model: {best_name}\")\n",
    "\n",
    "    best_model = models[best_name]\n",
    "\n",
    "    # Predict probabilities (binary case expected)\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        proba = best_model.predict_proba(X_unl)\n",
    "        if proba.ndim == 2 and proba.shape[1] == 2:\n",
    "            pos_proba = proba[:, 1]\n",
    "        else:\n",
    "            pos_proba = proba.max(axis=1)\n",
    "    else:\n",
    "        print(\"Model has no predict_proba; using predict and accepting all pseudo-labels (not recommended).\")\n",
    "        preds_all = best_model.predict(X_unl)\n",
    "        pos_proba = np.ones(len(preds_all))\n",
    "\n",
    "    preds = best_model.predict(X_unl)\n",
    "\n",
    "    accept_mask = pos_proba >= CONF_THRESH\n",
    "    accepted_idx = np.where(accept_mask)[0]\n",
    "    print(f\"Total unlabeled samples: {len(X_unl)} | High-confidence (>= {CONF_THRESH}): {len(accepted_idx)}\")\n",
    "\n",
    "    if len(accepted_idx) == 0:\n",
    "        print(\"No pseudo-labels passed the confidence threshold. Consider lowering the threshold.\")\n",
    "    else:\n",
    "        if len(accepted_idx) > MAX_PSEUDO:\n",
    "            accepted_idx = accepted_idx[:MAX_PSEUDO]\n",
    "            print(f\"Limiting pseudo-labels to first {MAX_PSEUDO} samples\")\n",
    "\n",
    "        X_pseudo = X_unl[accepted_idx]\n",
    "        y_pseudo = preds[accepted_idx]\n",
    "\n",
    "        # Save pseudo-labels (with original unlabeled metadata if present)\n",
    "        df_pseudo = df_unl.loc[df_unl.index[accepted_idx]].copy()\n",
    "        df_pseudo['pseudo_label'] = y_pseudo\n",
    "        df_pseudo['pseudo_proba'] = pos_proba[accepted_idx]\n",
    "        df_pseudo.to_csv(PSEUDO_OUTPUT, index=False)\n",
    "        print(f\"Saved {len(accepted_idx)} pseudo-labeled samples to: {PSEUDO_OUTPUT}\")\n",
    "\n",
    "        # Augment training data\n",
    "        X_train_arr = np.asarray(X_train)\n",
    "        y_train_arr = np.asarray(y_train)\n",
    "\n",
    "        X_aug = np.vstack([X_train_arr, X_pseudo])\n",
    "        y_aug = np.concatenate([y_train_arr, y_pseudo])\n",
    "\n",
    "        # Sample weights (original=1, pseudo=<1) - used if model.fit supports sample_weight\n",
    "        sample_weight = np.concatenate([np.ones(len(X_train_arr)), np.full(len(X_pseudo), PSEUDO_WEIGHT)])\n",
    "\n",
    "        # Retrain best_model on augmented data (try with sample_weight, fallback to without)\n",
    "        print(f\"Retraining {best_name} on augmented data (orig={len(X_train_arr)}, pseudo={len(X_pseudo)})...\")\n",
    "        try:\n",
    "            best_model.fit(X_aug, y_aug, sample_weight=sample_weight)\n",
    "            print(\"Retrained with sample_weight\")\n",
    "        except TypeError:\n",
    "            best_model.fit(X_aug, y_aug)\n",
    "            print(\"Retrained without sample_weight (estimator does not support it)\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred_new = best_model.predict(X_val)\n",
    "        new_val_f1 = f1_score(y_val, y_val_pred_new)\n",
    "        new_val_acc = accuracy_score(y_val, y_val_pred_new)\n",
    "        print(f\"Validation F1 after pseudo-label retrain: {new_val_f1:.4f}\")\n",
    "        print(f\"Validation Acc after pseudo-label retrain: {new_val_acc:.4f}\")\n",
    "\n",
    "        # Optionally update models dict with retrained model\n",
    "        models[best_name] = best_model\n",
    "\n",
    "        print(\"Pseudo-labeling iteration complete. Review saved pseudo-labels and validation metrics before repeating.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7f274",
   "metadata": {},
   "source": [
    "## Next Steps: Model Saving & FastAPI Frontend\n",
    "You will now save your retrained model and preprocessing pipeline, then scaffold a FastAPI backend for gene signature classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b6a4256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model, vectorizer, and PCA transformer for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Save retrained model and preprocessing pipeline for deployment\n",
    "import joblib\n",
    "joblib.dump(models[best_name], 'gene_signature_model.pkl')\n",
    "joblib.dump(vect, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(pca, 'pca_transformer.pkl')\n",
    "print('Saved model, vectorizer, and PCA transformer for deployment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a00fc0",
   "metadata": {},
   "source": [
    "## FastAPI Backend: Gene Signature Classification API\n",
    "The following code scaffolds a FastAPI backend that loads your saved model and preprocessing pipeline, and exposes a `/classify` endpoint for molecular sample classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9fe7370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI backend for gene signature classification\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import uvicorn\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model and pipeline\n",
    "model = joblib.load('gene_signature_model.pkl')\n",
    "vect = joblib.load('tfidf_vectorizer.pkl')\n",
    "pca = joblib.load('pca_transformer.pkl')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemm = WordNetLemmatizer()\n",
    "    alpha_or_numeric = \"[^a-zA-Z0-9- ]\"\n",
    "    pre_txt = re.sub(alpha_or_numeric, \" \", text)\n",
    "    pre_txt = pre_txt.lower()\n",
    "    sample_words = [wordnet_lemm.lemmatize(w) for w in pre_txt.split() if w not in stop_words and len(w)>1]\n",
    "    return ' '.join(sample_words)\n",
    "\n",
    "@app.post('/classify')\n",
    "async def classify_sample(sample: str = Form(...)):\n",
    "    cleaned = preprocess_text(sample)\n",
    "    X_vec = vect.transform([cleaned])\n",
    "    X_pca = pca.transform(X_vec.toarray())\n",
    "    pred = model.predict(X_pca)[0]\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        conf = float(model.predict_proba(X_pca)[0][1])\n",
    "    else:\n",
    "        conf = None\n",
    "    return JSONResponse({'predicted_signature': str(pred), 'confidence': conf})\n",
    "\n",
    "# To run:\n",
    "# uvicorn filename:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1407e64",
   "metadata": {},
   "source": [
    "## Simple HTML Frontend for Gene Signature Classification\n",
    "This HTML form lets users submit a molecular sample to your FastAPI backend and displays the predicted gene signature and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "89754c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML frontend code saved as index.html\n"
     ]
    }
   ],
   "source": [
    "# Save this as index.html and open in your browser\n",
    "html_code = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Gene Signature Classifier</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 40px; background: #f7f7f7; }\n",
    "        .container { max-width: 500px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px #ccc; }\n",
    "        h2 { text-align: center; }\n",
    "        textarea { width: 100%; height: 100px; margin-bottom: 15px; }\n",
    "        button { padding: 10px 20px; background: #007bff; color: #fff; border: none; border-radius: 4px; cursor: pointer; }\n",
    "        .result { margin-top: 20px; padding: 15px; background: #e9ffe9; border-radius: 6px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h2>Gene Signature Classifier</h2>\n",
    "        <form id=\"classifyForm\">\n",
    "            <label for=\"sample\">Paste molecular sample (text):</label><br>\n",
    "            <textarea id=\"sample\" name=\"sample\" required></textarea><br>\n",
    "            <button type=\"submit\">Classify</button>\n",
    "        </form>\n",
    "        <div id=\"result\" class=\"result\" style=\"display:none;\"></div>\n",
    "    </div>\n",
    "    <script>\n",
    "        document.getElementById('classifyForm').onsubmit = async function(e) {\n",
    "            e.preventDefault();\n",
    "            const sample = document.getElementById('sample').value;\n",
    "            const formData = new FormData();\n",
    "            formData.append('sample', sample);\n",
    "            const response = await fetch('http://127.0.0.1:8000/classify', {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            });\n",
    "            const data = await response.json();\n",
    "            document.getElementById('result').style.display = 'block';\n",
    "            document.getElementById('result').innerHTML = \n",
    "                `<b>Predicted Signature:</b> ${data.predicted_signature} <br>` +\n",
    "                `<b>Confidence:</b> ${data.confidence !== null ? data.confidence.toFixed(3) : 'N/A'}`;\n",
    "        };\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "with open('index.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_code)\n",
    "print('HTML frontend code saved as index.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
